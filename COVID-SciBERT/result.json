{
    "Title": "COVID-19 Transformer Model (SciBERT)",
    "Tags": ["Natural Language Processing"],
    "Architecture": "BERT",
    "Publisher": [["Tanmay Thakur","https://github.com/lordtt13"], ["Smoketrees","https://github.com/smoke-trees"], ["CYBINT","https://github.com/CYBINT-IN"]],
    "Problem Domain": "Text",
    "Model Format": "Transformer",
    "Language": "English",
    "Dataset": [["CORD-19","https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge"]],
    "Overview": "A BERT model trained on the unsupervised data present in the COVID research papers with a SciBERT base.",
    "Preprocessing": "preprocessing.html",
    "Link": "https://drive.google.com/file/d/1idk98WbPMOAwQlqDCLSl6fdlQdTR2JaD/view?usp=sharing",
    "Usage": "usage.html",
    "References": ["https://arxiv.org/abs/1810.04805"],
    "Input Shape": [["Tokenize using tokenizer"]],
    "Output Shape": [["Transformer decides"]],
    "Description": "BERT (Bidirectional Encoder Representations from Transformers) BERT applies the bidirectional training of Transformer, to language modelling. This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training, Bidirectional Training makes the model have a better sense of language than uni-directional language models. BERT makes use of Transformer which is an attention mechanism that learns contextual relations between words. Transformer includes two separate mechanisms — an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT’s goal is to generate a language model, only the encoder mechanism is necessary. BERT can be used for a wide variety of language tasks with little tweaks, This model accompolishes classification tasks such as sentiment analysis are done similarly to Next Sentence classification, by adding a classification layer on top of the Transformer output. SciBERT is a Pretrained Language Model for Scientific Text. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We enhanced the CORD vocabulary and made it into an MLM(Masked Language Model) to get the results. Before feeding word sequences into BERT, a fixed percentage of the words in each sequence are replaced with a [MASK] token. The model attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence."
}

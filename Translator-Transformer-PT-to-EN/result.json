{
    "Title": "Transformer Translator (Portugese to English)",
    "Tags": ["Natural Language Processing"],
    "Architecture": "Encoder-Decoder",
    "Publisher": [["Tanmay Thakur","https://github.com/lordtt13"], ["Smoketrees","https://github.com/smoke-trees"]],
    "Problem Domain": "Text",
    "Model Format": "Transformer",
    "Language": "English",
    "Dataset": [["TFDS","https://www.tensorflow.org/datasets/catalog/ted_hrlr_translate"]],
    "Overview": "An encoder decoder model used as a machine translator, used to translate sentences from portugese to english.",
    "Preprocessing": null,
    "Link": "https://drive.google.com/file/d/13wOWNqMzTCcK9rJntmELmCu3sP3jZfnh/view?usp=sharing",
    "Usage": "usage.html",
    "References": ["https://arxiv.org/abs/1706.03762"],
    "Input Shape": [["Encoding function given"]],
    "Output Shape": [["Decoding function given"]],
    "description": "Machine Translation is just translation carried out by computer. It is a very important application of DL in the modern world. This model is implementation of the original paper 'attention is all you need', done from scartch. This model translated between english and portugese languages. The attention mechanism was born to help memorize long source sentences in neural machine translation (NMT). Rather than building a single context vector out of the encoder's last hidden state, the secret sauce invented by attention is to create shortcuts between the context vector and the entire source input."
}
